{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will talk about the classification models performance evaluation methods: \n",
    "    - Receiver Operating Characteristic(ROC) Curve and\n",
    "    - Area Under the ROC Curve (AUC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classifiers generally don't return a simple “Yes-or-No” answer. \n",
    "- Mostly, a classification procedure will return a score along a range\n",
    "- Ideally, the positive instances score towards one end of the scale, and the negative examples towards the other end. \n",
    "- It is up to the analyst to set a **threshold** on that score that separates what is considered a POSITIVE class or a NEGATIVE class.  \n",
    "\n",
    "- The ROC Curve is a tool that helps to set the **best threshold**.\n",
    "\n",
    "![roc1](/images/roc1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TEASER_END -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To understand the concept, here is a simple example of Logistic Regression for classification of mice as **obese** or **not obese** reagarding their weights. \n",
    "- X-axis shows the **weights**\n",
    "- Y-axis shows the **probabilities** of the target variable (being obese)\n",
    "\n",
    "\n",
    "![roc2](/images/roc2.jpg)\n",
    "\n",
    "Example is taken from [this youtube channel](https://www.youtube.com/watch?v=xugjARegisk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to turn these probabilities into classification as **obese** or **not obese**\n",
    "- One way to classify the mice is to set a threshold at $0.5$\n",
    "- The samples with the probability over $0.5$ will be classified as **obese** and the ones below will be classified as **non-obese**\n",
    "\n",
    "![roc3](/images/roc3.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To evaluate the efectiveness of the Logistic Regression with the classification **threshold set to 0.5** we can test it with new samples (mice) that we know obese or not obese\n",
    "\n",
    "- The blue ones are **actual obese** and the red ones are **actual not obese**\n",
    "\n",
    "![roc4](/images/roc4.png)\n",
    "\n",
    "- When we look at the plot we see that \n",
    "    - 1 **actual obese**(blue) classified as **not obese** (under the threshold)\n",
    "    - 1 **actual not obese**(red) classified as **obese** (over the threshold)\n",
    "    - 3 actual obese and 3 actual not obese classified correctly\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a confusion matrix from these results\n",
    "\n",
    "![roc5](/images/roc5.png)\n",
    "\n",
    "- With these results we can evaluate the performance of this Logistic Regression with the threshold set to $0.5$\n",
    "    - `Sensitivity (Recall)`= $0.75$\n",
    "    - `Specifity` = $0.75$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What if we set the threshold to $0.1$ in order to be able classify all the **actual obese** samples as **obese**\n",
    "![roc6](/images/roc6.png)\n",
    "\n",
    "\n",
    "- When we move the threshold from $0.5$ to $0.1$, or visually thinking when we move the threshold bar towards the bottom\n",
    "    - we classify the samples which stay above the bar are as POSITIVE(1) and the ones below the bar as NEGATIVE(0)\n",
    "    - we give more chance to capture all the actual POSITIVE (actual obese) samples but \n",
    "    - some actual NEGATIVEs also stayed over the bar in the positive zone as a result\n",
    "    - we increased the number of **TRUE**(ly) predicted **POSITIVE**s and  **FALSE**(ly) predicted as **POSITIVE**s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create a confusion matrix from these results\n",
    "\n",
    "![roc7](/images/roc7.png)\n",
    "\n",
    "- With these results we can evaluate the performance of the Logistic Regression with the threshold set to \n",
    "$0.1$\n",
    "    - `Sensitivity (Recall)`= $1.00$\n",
    "    - `Specifity `= $0.5$\n",
    "    \n",
    "    \n",
    "    \n",
    "- We notice that True Positive Rate (Sensitivity) increased and True Negative Rate (Specifity) decreased\n",
    "- There is a trade of between True Positive Rate and True Negative Rate \n",
    "- When we change the threshold generally one of them increases and the other decreases\n",
    "\n",
    "\n",
    "### How can we find the optimal threshold?\n",
    "- We can't compute confusion matrix for all the threshoslds but libraries like Sklearn can bring us the ROC curves\n",
    "- ROC curves shows the results of all the possible thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristics (ROC CURVE)\n",
    "- (Name comes from radar signals Receiver Operating Characteristics)\n",
    "\n",
    "- A graph \n",
    "\n",
    "    - summarizes the performance of a binary classifier (two classes 1 or 0, positive or negative) **over all thresholds**\n",
    "\n",
    "    - compares and visualizes the tradeoff between the model’s sensitivity and specificity.\n",
    "    - generated by plotting **True Positive Rate** (y-axis) against **False Positive Rate** (x-axis)\n",
    "    \n",
    "\n",
    "- The values (x,y) of each point on the ROC curve changes by the classification threshold used for assigning observations to a given class\n",
    "\n",
    "- We cannot compute the ROC curve from a confusion matrix.\n",
    "\n",
    "\n",
    "\n",
    "- ROC curves have an attractive property: they are **insensitive to changes in class distribution**. \n",
    "- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. \n",
    "\n",
    "\n",
    "- Any performance metric that uses values from both columns (positive and negative) will be inherently sensitive to class skews. \n",
    "- Metrics such as **accuracy, precision, lift** and **F-score** use values from both columns of the confusion matrix. \n",
    "\n",
    "- As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. \n",
    "\n",
    "\n",
    "- ROC graphs are based upon True Positive Rate and False Positive Rate, in which each dimension is a strict columnar ratio, so do not depend on class distributions.\n",
    "\n",
    "\n",
    "\n",
    "- ROC curves can only be used to assess classiﬁers that return some conﬁdence score (or a probability) of prediction. - For example, logistic regression, neural networks, and decision trees (and ensemble models based on decision trees) can be assessed using ROC curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `True Positive Rate= True Positive / All Positives`\n",
    "- `False Positive Rate= False Positive/ All Negatives`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's try to answer the question: **\"How can we find the optimal threshold?\"** by checking 3 thresholds (points) on the ROC curve below.\n",
    "\n",
    "\n",
    "![roc8](/images/roc8.png)\n",
    "\n",
    "(By the way, any ROC curve generated from a finite set of samples is actually a step function, like above, which approaches a true curve as the number of instances approaches infinity)\n",
    "\n",
    "- Threshold1: At the bottom-left, point $(0,0)$ (The threshold bar is at the top)\n",
    "    - False Positive Rate (FPR):$0$ This is great. The classifier did not make identified any actual Negative sample as Positive \n",
    "    - True Positive Rate (TPR): $0$ This is awful. The classifier could not catch any of the True Positive samples\n",
    "    \n",
    "\n",
    "\n",
    "- Threshold2: At the top-left corner, point $(0.4, 0.8)$   \n",
    "    - FPR:$0.4$ Classifier identified some negative samples as positive\n",
    "    - TPR:$0.8$ Classifier showed a good performance on capturing the actual positives\n",
    "    \n",
    "    \n",
    "    \n",
    "- Threshold3: At the top-right, point $(1.0, 1.0)$ (The thereshold bar is at the bottom)\n",
    "    - FPR:$1.0$ This is awful. Classifier identified all the actual negative samples as positive\n",
    "    - TPR:$1.0$ Classifier showed a good performance on capturing all the actual positives\n",
    "    \n",
    "    \n",
    "    \n",
    "- So, the ideal point is therefore the top-left corner of the plot: false positives are close to $0$ and true positives are close $1$.\n",
    "\n",
    "- Now, we know how to choose the optimal threshold for a classifier by ROC curve but **how can we choose the best performing model with ROC curves?**: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After plotting the ROC curves of some models we inspect\n",
    "    - the top-left corner values on the curves and\n",
    "    - the “steepness” of the curve, as this describes the **maximization of the true positive rate** while **minimizing the false positive rate**.\n",
    "\n",
    "![roc9](/images/roc9.png)\n",
    "\n",
    "- This leads to another metric, **Area Under the Curve (AUC)**, which is a computation of the relationship between false positives and true positives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Under the ROC Curve (AUC)\n",
    "- To compare the optimal performances of different classifiers we can use the size of the area of each classifiers ROC curves\n",
    "\n",
    "- As the picture displays the bigger area under the curve means the model of that curve can classify the actual positive and actual negatives better.\n",
    "\n",
    "- Think of the red and green curves(distributions) like this: \n",
    "    - Firstly, they are not ROC curves, ROC curves are on the right side\n",
    "    - We gave a validation dataset to some models and\n",
    "    - models gave us the predict probability of each sample (e.g Sklearn with `predict_proba`)\n",
    "    - Since we know the labels of them. We assosiated **green for the positives** and **red for the negatives**\n",
    "    - We noticed that some models could separate two classes better than other. \n",
    "    - The more overlapping the worse classification performance\n",
    "\n",
    "![roc10](/images/roc10.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How to Interpret AUC?\n",
    "\n",
    "![roc11](/images/roc11.png)\n",
    "\n",
    "\n",
    "- AUC provides an **aggregate measure** of performance **across all possible classification thresholds**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's try to interpret a model's AUC score which has a ROC curve like below:\n",
    "\n",
    "![roc12](/images/roc12.png)\n",
    "\n",
    "- For all the thresholds the values of **True Positive Rate (TPR)** and the **False Positive Rate(FPR)** (the proportion of negative examples predicted incorrectly) will be equal on the curve (The diagonal line **y=x**) \n",
    "\n",
    "- In order to get away from this diagonal into the upper triangular region, the classifier must exploit some information in the data. \n",
    "\n",
    "\n",
    "\n",
    "- Any classifier that appears in the lower right triangle performs worse than random guessing. \n",
    "\n",
    "- This triangle is therefore usually empty in ROC graphs. \n",
    "\n",
    "\n",
    "\n",
    "- If we negate a classifier that is, reverse its classification decisions on every instance its true positive classifications become false negative mistakes, and its false positives become true negatives. \n",
    "\n",
    "- Therefore, any classifier that produces a point in the lower right triangle can be negated to produce a point in the upper left triangle. \n",
    "\n",
    "\n",
    "\n",
    "- Any classifier on the diagonal may be said to have no information about the class. \n",
    "\n",
    "- A classifier below the diagonal may be said to have useful information, but it is applying the information incorrectly \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We look for the model performance where True Positive Rate is significantly higher than False Positive Rate.\n",
    "- This means that our ROC curve should satisfy the condition **y > x**\n",
    "- This is the region of upper triangle\n",
    "\n",
    "![roc13](/images/roc13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is AUC a desirable metric? \n",
    "\n",
    "- AUC is **scale-invariant**. \n",
    "    - It measures how well predictions are ranked, rather than their absolute values.\n",
    "    - AUC is based on the relative predictions, so any transformation of the predictions that preserves the relative ranking has no effect on AUC. \n",
    "    \n",
    "    \n",
    "- AUC is **classification-threshold-invariant**. \n",
    "    - It measures the quality of the model's predictions irrespective of what classification threshold is chosen.\n",
    "    \n",
    "    - The amount of spread between predictions does not actually impact AUC. Even a prediction score for a randomly drawn true positive is only a tiny epsilon greater than a randomly drawn negative, that will count that as a success contributing to the overall AUC score.\n",
    "    \n",
    "\n",
    "However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:\n",
    "- Scale invariance is not always desirable. \n",
    "- For example, sometimes we really do need **well calibrated probability outputs**, and AUC won’t tell us about that.\n",
    "\n",
    "- **Classification-threshold invariance** is not always desirable. In cases where there are **wide disparities in the cost of false negatives vs. false positives**, it may be critical to minimize one type of classification error. \n",
    "\n",
    "\n",
    "### When AUC is not  suitable?\n",
    "\n",
    "For example, when doing email spam detection, \n",
    "- you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). \n",
    "- AUC isn't a useful metric for this type of optimization.\n",
    "\n",
    "- In practice, if you have a \"perfect\" classifier with an AUC of 1.0, you should be suspicious, as it likely indicates a bug in your model. \n",
    "\n",
    "- For example, you may have overfit to your training data, or the label data may be replicated in one of your features\n",
    "\n",
    "\n",
    "- Without an explicit cost of error model (cost of false positives and separate cost of false negatives) you should always be suspicious of a single number summary of a classifier performance (be it accuracy, AUC, F1 or so on). \n",
    "\n",
    "- We in fact prefer using both precision and recall. If you insist on a single number: the F1 is a good heuristic measure of classifier quality, as it at least incorporates our operational choice of score threshold into the quality assessment. \n",
    "\n",
    "- The ROC curve is useful tool designing a classifier from a scoring function (though I prefer the “double hump graph”), but once you have chosen a threshold the performance of the other classifiers (induced by choosing different thresholds) are irrelevant to assessing the performance of the classifier you have settled on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: <br>\n",
    "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc <br>\n",
    "\n",
    "https://stackoverflow.com/questions/19984957/scikit-predict-default-threshold <br>\n",
    "\n",
    "https://www.kaggle.com/kevinarvai/fine-tuning-a-classifier-in-scikit-learn <br>\n",
    "https://stackoverflow.com/questions/31417487/sklearn-logisticregression-and-changing-the-default-threshold-for-classification?rq=1 <br>\n",
    "\n",
    "https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65 <br>\n",
    "\n",
    "https://people.inf.elte.hu/kiss/11dwhdm/roc.pdf <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nikola": {
   "category": "",
   "date": "2018-10-16 22:08:04 UTC+02:00",
   "description": "",
   "link": "",
   "slug": "roc curve & auc",
   "tags": "roc curve, area under the roc curve, auc",
   "title": "ROC Curve & AUC",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
