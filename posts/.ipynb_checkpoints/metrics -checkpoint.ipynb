{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously it is of great importance to understand and utilize the metrics properly also in machine learning. Deriving insights without making clear sense of metrics is like choosing between 1 litre of milk and 0.6 galon of milk. If we dont know well the metrics litre and galon we can't make an healty decision.\n",
    "\n",
    "In this notebook we try to understand the machine learning. We will regularly add new metrics to this notebook by time.\n",
    "![ruler](/images/ruler.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TEASER_END -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Squared ( $R^2$) and Mean Squared Error ($MSE$)\n",
    "In the previous post we cover the metric Mean Squared Logarithmic error. Let's now review the metrics R-squared and Mean Squared Error\n",
    "\n",
    "![mse_r2](/images/mse_r2.png)\n",
    "\n",
    "- $MSE$ measures how far the data are from the model’s predicted values \n",
    "\n",
    "- $R^2$ measures how far the data are from the model’s predicted values compare to how far the data are from the mean\n",
    "\n",
    "- The difference between how far the data are from the model’s predicted values  and how far the data are from the mean is the improvement in prediction from the regression model. \n",
    "\n",
    "### $MSE$\n",
    "\n",
    "- Sensitive to outliers\n",
    "- Has the same units as the response variable. \n",
    "- Lower values of $MSE$ indicate better fit. \n",
    "- Actually, it’s hard to realize if our model is good or not by looking at the absolute values of $MSE$ or $MSE$. \n",
    "- We would probably want to measure how much our model is better than the constant baseline.\n",
    "\n",
    "**Disadvantage of MSE**: \n",
    "- If we make a single very bad prediction, the squaring will make the error even worse and\n",
    "- it may skew the metric towards overestimating the model’s badness. \n",
    "- That is a particularly problematic behaviour if we have noisy data (data that for whatever reason is not entirely reliable) \n",
    "- On the other hand, if all the errors are **smaller than 1**, than the opposite effect is felt: we may underestimate the model’s badness.\n",
    "\n",
    "###  $R^2$\n",
    "\n",
    "- proportional improvement in prediction of the **regression model**, compared to the **mean model** (model predicting all given samples as mean value).\n",
    "\n",
    "- interpreted as the proportion of total variance that is explained by the model.\n",
    "\n",
    "-  **relative measure** of fit whereas $MSE$ is an **absolute measure** of fit\n",
    "- often easier to interpret since it doesn't depend on the scale of the data.\n",
    "    - It doesn’t matter if the output values are very large or very small, \n",
    "-  always has values between `-∞` and `1`.\n",
    "\n",
    "- There are situations in which a high $R^2$ is not necessary or relevant. \n",
    "- When the interest is in the **relationship between variables**, not in prediction, the $R^2$ is less important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Logaritmic Error (RMSLE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cost](/images/cost.jpg)\n",
    "\n",
    "\n",
    "**Mechanism**: \n",
    "\n",
    "- It is the Root Mean Squared Error of the **log-transformed predicted** and **log-transformed actual values**. \n",
    "- `RMSLE` adds `1` to both actual and predicted values before taking the natural logarithm to avoid taking the natural log of possible `0 (zero)` values. \n",
    "\n",
    "- As a result, the function can be used if actual or predicted have **zero-valued** elements. But this function is not appropriate if either is **negative valued**\n",
    "\n",
    "**Functionality:**\n",
    "\n",
    "- The expression $$ log(p_i +1) − log(a_i+1)$$ can be written as $$ log((p_i+1) / (a_i+1)) $$ \n",
    "\n",
    "- RMSLE measures the ratio of predicted and actual.\n",
    "\n",
    ">RMSLE is preferable when \n",
    ">- targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc \n",
    ">- we care about **percentage errors** rather than the **absolute value of errors**.\n",
    ">- there is a wide range in the target variables and\n",
    ">- we **don’t want to penalize big differences** when **both the predicted and the actual are big numbers**.\n",
    ">- we want to penalize **under estimates** more than **over estimates**.\n",
    "\n",
    "Let's imagine two cases of predictions, \n",
    "- Case-1: our model predicts the number of bike rentals as `30` when the actual number is `40` \n",
    "- Case-2: our model predicts the number of rentals as `300` when the actual number is `400`\n",
    "\n",
    "- With RMSE the second result is scored as `10 times` more than the first result\n",
    "- Conversely, with RMSLogE two results are scored the same. \n",
    "- RMSLogE takes into account just the ratio of change\n",
    "\n",
    "The decisions always depends on the domains or business cases and for the bike rentals business and the ranges of bike rentals this **\"ratio of change\"** feature of RMSLE looks like suitable.\n",
    "<br>\n",
    "\n",
    "\n",
    "Lets have a look at the below example\n",
    "\n",
    "- Case-3 : \n",
    "    - Prediction = $600$, Actual = $1000$ (the absolute difference is $400$)\n",
    "\n",
    "    - RMSE = $400$, \n",
    "    - RMSLogE = $0.5108$\n",
    "\n",
    "\n",
    "\n",
    "- Case-4 : \n",
    "    - Prediction = $1400$, Actual = $1000$ (the absolute difference is $400$)\n",
    "\n",
    "    - RMSE = $400$,\n",
    "    - RMSLogE = $0.3365$\n",
    "    \n",
    "    \n",
    "\n",
    "- When the differences are the same between **actual** and **predicted** in both cases. \n",
    "    - RMSE treated them equally, however \n",
    "    - RMSLogE penalized the under estimate more than over estimate (under estimated prediction score is higher than over estimated prediction score)\n",
    "    \n",
    "- This second feature (**penalizing the under estimate more than over estimate**) of RSMLE also looks plausible for bike business.\n",
    "- Having extra bikes might be preferable to not being able to supply as many bikes as the demand. \n",
    "- Often **penalizing the under estimate more than over estimate** is important for prediction of sales and inventory demands."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nikola": {
   "category": "",
   "date": "2018-10-08 22:08:04 UTC+02:00",
   "description": "",
   "link": "",
   "slug": "metrics",
   "tags": "scoring metrics, mean squared error, mse, r-squared, root mean squared logaritmic error, rmsle",
   "title": "Scoring Metrics",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
