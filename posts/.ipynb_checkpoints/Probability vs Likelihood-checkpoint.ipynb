{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand these concepts better, let's walk through one of the simplest random variable example: coin tossing\n",
    "\n",
    "- If we flip a coin `100` times and it comes up head `53` times then we could have a strong feeling that the coin is fair. \n",
    "- How would we feel about the fairness of the coin if we see `30` heads `70` tails after `100` tosses? \n",
    "- Can we trust the coin? \n",
    "\n",
    "- To answer this question, we need to ask another one: **How much our sample support our hypothesis that `P(H)=P(T)=1/2`?** \n",
    "- Note that the direction of the question has reversed.\n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "- (If our coin is **fair** then the probability that it will come up `head` is `0.5`) \n",
    "\n",
    "\n",
    "- We know that it is also possible to see this outcome with a fair coin but the probabilty of this result is very low.\n",
    "\n",
    "- We can calculate the probability of `30` heads `70` tails after `100` tosses with a fair coin by the formula below and a binom calculator\n",
    "\n",
    "![binom1](/images/binom1.png)\n",
    "\n",
    "- `n`: number of trials\n",
    "- `p`: probability of success\n",
    "- `x`: number of success out of `n` trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![binom2](/images/binom2.png)\n",
    "\n",
    "- Here we assumed that the coin was fair and `P(head)= 0.5`\n",
    "- The probability of observing `30` and less number of heads with a **fair** coin is `0.000039`\n",
    "- Regarding this probability now have a reasonable basis for the **suspicion** that the coin is not fair. \n",
    "\n",
    "\n",
    "- In general, we calculate the probability of observing a particular set of outcomes by making suitable assumptions about the underlying process (e.g. probability of a coin landing heads is `p` and that coin tosses are `independent`).\n",
    "\n",
    "\n",
    "- So we look for the probabilities **when the parameters are known**. In our example `p(head) = p(tail)= 1/2`\n",
    "- We can denote it more generally as:\n",
    "     - observed **outcomes** by `O` and \n",
    "     - set of **parameters** that describe the distribution(stochastic process) as `θ`\n",
    "     \n",
    "     \n",
    "\n",
    "- So, given specific values for `θ`, `P(O|θ)` is the probability that we would observe the outcomes represented by `O`.\n",
    "- Here, we know that given a value of parameter `θ` the probability of observing data `O` is `P(O|θ)`.\n",
    "- However, when we model a real life stochastic process, **we often do not know `θ`**.\n",
    "\n",
    "\n",
    "- Suppose somebody is trying to convince us to play a gambling game\n",
    "- Before jumping in the gambling game, we can use the **probabilities** to compute properties like the **expected gains** and **loses** (mean, mode, median, variance, information ratio, value at risk etc)\n",
    "\n",
    "\n",
    "\n",
    "- Here, **likelihood** will help us to determine if we trust those **probabilities**.\n",
    "\n",
    "- We simply observe data and the goal is to arrive at an **estimate for model parameters** (in our coin toss example the probability of observing head or tail) that would be a plausible choice.\n",
    "\n",
    "\n",
    "- Again, we are given a **fixed data set**(the measurement) and we are postulating the distribution that data set came from.\n",
    "- In other words, we find the **parameter values `θ`** that **maximize the probability** that we would actually observe our data `O`,  maximize the `L(θ|O)` function \n",
    "\n",
    "- `L(θ|O)` is called the likelihood function. \n",
    "\n",
    "\n",
    "- Notice that likelihood is a function of the **unknown parameters `θ`** of the distribution, given a sample from that distribution.\n",
    "\n",
    "- Likelihood deals with fitting models given some **known data** \n",
    "- Likelihood is a measure calculated from a data sample to provide support for particular values of a parameter in a parametric model. \n",
    "\n",
    "- To have low values of likelihood means either we observe a **rare data** or an **incorrect model**!\n",
    "\n",
    "\n",
    "- One could say that statistics comprises of two parts: \n",
    "    - Question of how to formulate and evaluate probabilistic models for the problem\n",
    "    - Question of obtaining answers after a certain model has been assumed\n",
    "\n",
    "\n",
    "- Statistical questions can be converted to probability questions by the use of **probability models**. \n",
    "- Once we make certain assumptions about the **mechanism that generates the data**, we can answer statistical questions using probability theory. \n",
    "- However, the proper **formulation** and **checking of these probability models** is just as important, or even more important, than the subsequent analysis of the problem using these models.\n",
    "\n",
    "\n",
    "\n",
    "- **Probability** quantifies anticipation of outcome from a know distribution, \n",
    "- **Likelihood** quantifies trust in the model from which we assume the observed data is coming \n",
    "-  Likelihood is often used as an **objective function**, but also as a **performance measure** to compare two models \n",
    "\n",
    "\n",
    "#### More formally\n",
    "- A statistical model has to connect two distinct conceptual entities: \n",
    "    - **data**, which are elements `x` of some set (such as a vector space), and \n",
    "    - **model** of the data behavior. \n",
    "    \n",
    "    \n",
    "- The data `x` are connected to the possible models with parameter `θ` by means of a function `Λ(x,θ)`. \n",
    "\n",
    "    - For any `given θ`, `Λ(x,θ)` is intended to be the **probability** (or **probability density**) of `x`. \n",
    "    - For any `given x`, on the other hand, `Λ(x,θ)` can be viewed as a **function of `θ` (likelihood)** and is usually assumed to have certain properties, such as being continuous, second differentiable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual explanation of likelihood\n",
    "- As an example, if we have some data from a normal distribution, but we don't know the **mean** and **standard deviation** of that normal distribution, we can use **maximum likelihood** to determine the **optimal estimates** for the mean and standard deviation.  \n",
    "\n",
    "- The slides below were taken from the great [Youtube channel StatQuest](https://www.youtube.com/watch?v=XepXtl9YKwc&t=11s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![like1](/images/like1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of **maximum likelihood** is to find the **optimum way to fit a distribution to the data**\n",
    "![like2](/images/like2.jpg)\n",
    "\n",
    "The reasons we want to fit a distribution to our data:\n",
    "- it can be easier to work with \n",
    "- it is also more general (we can apply to every experiment of the same type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![like3](/images/like3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once we decided on the distribution, it is time to figure out the center.\n",
    "- Is one location is better than others as a center candidate?\n",
    "\n",
    "![like4](/images/like4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![like5](/images/like5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![like6](/images/like6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![like7](/images/like7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![like8](/images/like8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![like9](/images/like9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![like10](/images/like10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![like11](/images/like11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![like12](/images/like12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the likelihood means trying to find the optimal value for the mean or the standard deviation for a distribution given a observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to calculate the likelihood of a normal distribution?\n",
    "- When we’re working with a discrete distribution, like the binomial distribution, then likelihood and probability are calculated the same way and we get the same value for both. \n",
    "\n",
    "- However, with a continuous distribution, like the normal distribution, then they are quite different. \n",
    "\n",
    "- If our function is continuous function thus a probability density function (PDF):\n",
    "\n",
    "    - The likelihood for a **single data point** is the y-axis value that corresponds to that point.\n",
    "\n",
    "    - The probability, however, is calculated as the integral between two points. So for a single data point, the probability is always `zero`.\n",
    "\n",
    "    - If we have multiple measurements (data), the likelihood is the various y-axis values multiplied together.\n",
    "\n",
    "    - Likelihood values can be greater than 1.\n",
    "\n",
    "\n",
    "#### What is on the y-axis of a PDF?\n",
    "- For example, suppose the graph is the distribution of **height** of adult males in **centimeters**. \n",
    "\n",
    "- Then the y-axis units are `probability / centimeters`. \n",
    "\n",
    "- Then when you multiply a **y-axis value** (`p/cm`) times an **x-axis value** (`cm`) to get an **area** (`p`), the units are **probability**.\n",
    "\n",
    "- That unit doesn’t make a lot of sense at first\n",
    "- We define the **probability density function** as the derivative of the **cumulative distribution function—F(X)**. \n",
    "- The cumulative distribution function is measured in units of probability, so it’s easy to describe, F(X) is the **probability of a random observation** being **less than or equal to X**.\n",
    "\n",
    "\n",
    "- Say the value of `f(X)` is `0.01` at `175`. \n",
    "- What does that mean? \n",
    "- It means that for a small range in centimeters around `175`, say from `174.9` to `175.1`, the probability of a random adult male being found in the range is about `0.01` times the width of the range in centimeters, so `0.01*(175.1 - 174.9) = 0.002`\n",
    "\n",
    "- So we think about `0.2%` of adult men will have a height in that range. \n",
    "\n",
    "- If we measured in inches instead of centimeters, the value of f(X) would be `0.0254` instead of `0.01`, because the new height units are `2.54` larger than the old height units.\n",
    "\n",
    "- So the measured values are divided by 2.54, so f(X) which is measured in inverse unit will be 2.54 as large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability#2647\n",
    "- https://www.youtube.com/watch?v=XepXtl9YKwc&t=11s \n",
    "- https://www.quora.com/Is-there-a-Y-axis-on-a-bell-curve-or-the-normal-distribution-If-yes-what-does-it-denote-If-not-then-why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nikola": {
   "category": "",
   "date": "2017-10-08 22:08:04 UTC+02:00",
   "description": "",
   "link": "",
   "slug": "likelihood",
   "tags": "probability, likelihood",
   "title": "Probability vs Likelihood",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
