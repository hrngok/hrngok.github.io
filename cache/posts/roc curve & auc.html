
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>We will talk about the classification models performance evaluation methods: <ul>
<li>Receiver Operating Characteristic(ROC) Curve and</li>
<li>Area Under the ROC Curve (AUC)</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Classifiers generally don't return a simple “Yes-or-No” answer. </li>
<li>Mostly, a classification procedure will return a score along a range</li>
<li>Ideally, the positive instances score towards one end of the scale, and the negative examples towards the other end. </li>
<li><p>It is up to the analyst to set a <strong>threshold</strong> on that score that separates what is considered a POSITIVE class or a NEGATIVE class.</p>
</li>
<li><p>The ROC Curve is a tool that helps to set the <strong>best threshold</strong>.</p>
</li>
</ul>
<p><img src="/images/roc1.png" alt="roc1"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<!-- TEASER_END -->
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>To understand the concept, here is a simple example of Logistic Regression for classification of mice as <strong>obese</strong> or <strong>not obese</strong> reagarding their weights. </li>
<li>X-axis shows the <strong>weights</strong></li>
<li>Y-axis shows the <strong>probabilities</strong> of the target variable (being obese)</li>
</ul>
<p><img src="/images/roc2.png" alt="roc2"></p>
<p>Example is taken from <a href="https://www.youtube.com/watch?v=xugjARegisk">this youtube channel</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>We need to turn these probabilities into classification as <strong>obese</strong> or <strong>not obese</strong></li>
<li>One way to classify the mice is to set a threshold at $0.5$</li>
<li>The samples with the probability over $0.5$ will be classified as <strong>obese</strong> and the ones below will be classified as <strong>non-obese</strong></li>
</ul>
<p><img src="/images/roc3.png" alt="roc3"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><p>To evaluate the efectiveness of the Logistic Regression with the classification <strong>threshold set to 0.5</strong> we can test it with new samples (mice) that we know obese or not obese</p>
</li>
<li><p>The blue ones are <strong>actual obese</strong> and the red ones are <strong>actual not obese</strong></p>
</li>
</ul>
<p><img src="/images/roc4.png" alt="roc4"></p>
<ul>
<li>When we look at the plot we see that <ul>
<li>1 <strong>actual obese</strong>(blue) classified as <strong>not obese</strong> (under the threshold)</li>
<li>1 <strong>actual not obese</strong>(red) classified as <strong>obese</strong> (over the threshold)</li>
<li>3 actual obese and 3 actual not obese classified correctly</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let's create a confusion matrix from these results</li>
</ul>
<p><img src="/images/roc5.png" alt="roc5"></p>
<ul>
<li>With these results we can evaluate the performance of this Logistic Regression with the threshold set to $0.5$<ul>
<li><code>Sensitivity (Recall)</code>= $0.75$</li>
<li><code>Specifity</code> = $0.75$</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>What if we set the threshold to $0.1$ in order to be able classify all the <strong>actual obese</strong> samples as <strong>obese</strong>
<img src="/images/roc6.png" alt="roc6"></li>
</ul>
<ul>
<li>When we move the threshold from $0.5$ to $0.1$, or visually thinking when we move the threshold bar towards the bottom<ul>
<li>we classify the samples which stay above the bar are as POSITIVE(1) and the ones below the bar as NEGATIVE(0)</li>
<li>we give more chance to capture all the actual POSITIVE (actual obese) samples but </li>
<li>some actual NEGATIVEs also stayed over the bar in the positive zone as a result</li>
<li>we increased the number of <strong>TRUE</strong>(ly) predicted <strong>POSITIVE</strong>s and  <strong>FALSE</strong>(ly) predicted as <strong>POSITIVE</strong>s</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let's create a confusion matrix from these results</li>
</ul>
<p><img src="/images/roc7.png" alt="roc7"></p>
<ul>
<li>With these results we can evaluate the performance of the Logistic Regression with the threshold set to 
$0.1$<ul>
<li><code>Sensitivity (Recall)</code>= $1.00$</li>
<li><code>Specifity</code>= $0.5$</li>
</ul>
</li>
</ul>
<ul>
<li>We notice that True Positive Rate (Sensitivity) increased and True Negative Rate (Specifity) decreased</li>
<li>There is a trade of between True Positive Rate and True Negative Rate </li>
<li>When we change the threshold generally one of them increases and the other decreases</li>
</ul>
<h3 id="How-can-we-find-the-optimal-threshold?">How can we find the optimal threshold?<a class="anchor-link" href="#How-can-we-find-the-optimal-threshold?">&#182;</a></h3><ul>
<li>We can't compute confusion matrix for all the threshoslds but libraries like Sklearn can bring us the ROC curves</li>
<li>ROC curves shows the results of all the possible thresholds</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Receiver-Operating-Characteristics-(ROC-CURVE)">Receiver Operating Characteristics (ROC CURVE)<a class="anchor-link" href="#Receiver-Operating-Characteristics-(ROC-CURVE)">&#182;</a></h2><ul>
<li><p>(Name comes from radar signals Receiver Operating Characteristics)</p>
</li>
<li><p>A graph</p>
<ul>
<li><p>summarizes the performance of a binary classifier (two classes 1 or 0, positive or negative) <strong>over all thresholds</strong></p>
</li>
<li><p>compares and visualizes the tradeoff between the model’s sensitivity and specificity.</p>
</li>
<li>generated by plotting <strong>True Positive Rate</strong> (y-axis) against <strong>False Positive Rate</strong> (x-axis)</li>
</ul>
</li>
</ul>
<ul>
<li><p>The values (x,y) of each point on the ROC curve changes by the classification threshold used for assigning observations to a given class</p>
</li>
<li><p>We cannot compute the ROC curve from a confusion matrix.</p>
</li>
</ul>
<ul>
<li>ROC curves have an attractive property: they are <strong>insensitive to changes in class distribution</strong>. </li>
<li>If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. </li>
</ul>
<ul>
<li>Any performance metric that uses values from both columns (positive and negative) will be inherently sensitive to class skews. </li>
<li><p>Metrics such as <strong>accuracy, precision, lift</strong> and <strong>F-score</strong> use values from both columns of the confusion matrix.</p>
</li>
<li><p>As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not.</p>
</li>
</ul>
<ul>
<li>ROC graphs are based upon True Positive Rate and False Positive Rate, in which each dimension is a strict columnar ratio, so do not depend on class distributions.</li>
</ul>
<ul>
<li>ROC curves can only be used to assess classiﬁers that return some conﬁdence score (or a probability) of prediction. - For example, logistic regression, neural networks, and decision trees (and ensemble models based on decision trees) can be assessed using ROC curves. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><code>True Positive Rate= True Positive / All Positives</code></li>
<li><code>False Positive Rate= False Positive/ All Negatives</code></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let's try to answer the question: <strong>"How can we find the optimal threshold?"</strong> by checking 3 thresholds (points) on the ROC curve below.</li>
</ul>
<p><img src="/images/roc8.png" alt="roc8"></p>
<p>(By the way, any ROC curve generated from a finite set of samples is actually a step function, like above, which approaches a true curve as the number of instances approaches infinity)</p>
<ul>
<li>Threshold1: At the bottom-left, point $(0,0)$ (The threshold bar is at the top)<ul>
<li>False Positive Rate (FPR):$0$ This is great. The classifier did not make identified any actual Negative sample as Positive </li>
<li>True Positive Rate (TPR): $0$ This is awful. The classifier could not catch any of the True Positive samples</li>
</ul>
</li>
</ul>
<ul>
<li>Threshold2: At the top-left corner, point $(0.4, 0.8)$   <ul>
<li>FPR:$0.4$ Classifier identified some negative samples as positive</li>
<li>TPR:$0.8$ Classifier showed a good performance on capturing the actual positives</li>
</ul>
</li>
</ul>
<ul>
<li>Threshold3: At the top-right, point $(1.0, 1.0)$ (The thereshold bar is at the bottom)<ul>
<li>FPR:$1.0$ This is awful. Classifier identified all the actual negative samples as positive</li>
<li>TPR:$1.0$ Classifier showed a good performance on capturing all the actual positives</li>
</ul>
</li>
</ul>
<ul>
<li><p>So, the ideal point is therefore the top-left corner of the plot: false positives are close to $0$ and true positives are close $1$.</p>
</li>
<li><p>Now, we know how to choose the optimal threshold for a classifier by ROC curve but <strong>how can we choose the best performing model with ROC curves?</strong>:</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>After plotting the ROC curves of some models we inspect<ul>
<li>the top-left corner values on the curves and</li>
<li>the “steepness” of the curve, as this describes the <strong>maximization of the true positive rate</strong> while <strong>minimizing the false positive rate</strong>.</li>
</ul>
</li>
</ul>
<p><img src="/images/roc9.png" alt="roc9"></p>
<ul>
<li>This leads to another metric, <strong>Area Under the Curve (AUC)</strong>, which is a computation of the relationship between false positives and true positives</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Area-Under-the-ROC-Curve-(AUC)">Area Under the ROC Curve (AUC)<a class="anchor-link" href="#Area-Under-the-ROC-Curve-(AUC)">&#182;</a></h2><ul>
<li><p>To compare the optimal performances of different classifiers we can use the size of the area of each classifiers ROC curves</p>
</li>
<li><p>As the picture displays the bigger area under the curve means the model of that curve can classify the actual positive and actual negatives better.</p>
</li>
<li><p>Think of the red and green curves(distributions) like this:</p>
<ul>
<li>Firstly, they are not ROC curves, ROC curves are on the right side</li>
<li>We gave a validation dataset to some models and</li>
<li>models gave us the predict probability of each sample (e.g Sklearn with <code>predict_proba</code>)</li>
<li>Since we know the labels of them. We assosiated <strong>green for the positives</strong> and <strong>red for the negatives</strong></li>
<li>We noticed that some models could separate two classes better than other. </li>
<li>The more overlapping the worse classification performance</li>
</ul>
</li>
</ul>
<p><img src="/images/roc10.png" alt="roc10"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="How-to-Interpret-AUC?">How to Interpret AUC?<a class="anchor-link" href="#How-to-Interpret-AUC?">&#182;</a></h3><p><img src="/images/roc11.png" alt="roc11"></p>
<ul>
<li>AUC provides an <strong>aggregate measure</strong> of performance <strong>across all possible classification thresholds</strong>. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's try to interpret a model's AUC score which has a ROC curve like below:</p>
<p><img src="/images/roc12.png" alt="roc12"></p>
<ul>
<li><p>For all the thresholds the values of <strong>True Positive Rate (TPR)</strong> and the <strong>False Positive Rate(FPR)</strong> (the proportion of negative examples predicted incorrectly) will be equal on the curve (The diagonal line <strong>y=x</strong>)</p>
</li>
<li><p>In order to get away from this diagonal into the upper triangular region, the classifier must exploit some information in the data.</p>
</li>
</ul>
<ul>
<li><p>Any classifier that appears in the lower right triangle performs worse than random guessing.</p>
</li>
<li><p>This triangle is therefore usually empty in ROC graphs.</p>
</li>
</ul>
<ul>
<li><p>If we negate a classifier that is, reverse its classification decisions on every instance its true positive classifications become false negative mistakes, and its false positives become true negatives.</p>
</li>
<li><p>Therefore, any classifier that produces a point in the lower right triangle can be negated to produce a point in the upper left triangle.</p>
</li>
</ul>
<ul>
<li><p>Any classifier on the diagonal may be said to have no information about the class.</p>
</li>
<li><p>A classifier below the diagonal may be said to have useful information, but it is applying the information incorrectly</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>We look for the model performance where True Positive Rate is significantly higher than False Positive Rate.</li>
<li>This means that our ROC curve should satisfy the condition <strong>y &gt; x</strong></li>
<li>This is the region of upper triangle</li>
</ul>
<p><img src="/images/roc13.png" alt="roc13"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Why-is-AUC-a-desirable-metric?">Why is AUC a desirable metric?<a class="anchor-link" href="#Why-is-AUC-a-desirable-metric?">&#182;</a></h3><ul>
<li>AUC is <strong>scale-invariant</strong>. <ul>
<li>It measures how well predictions are ranked, rather than their absolute values.</li>
<li>AUC is based on the relative predictions, so any transformation of the predictions that preserves the relative ranking has no effect on AUC. </li>
</ul>
</li>
</ul>
<ul>
<li><p>AUC is <strong>classification-threshold-invariant</strong>.</p>
<ul>
<li><p>It measures the quality of the model's predictions irrespective of what classification threshold is chosen.</p>
</li>
<li><p>The amount of spread between predictions does not actually impact AUC. Even a prediction score for a randomly drawn true positive is only a tiny epsilon greater than a randomly drawn negative, that will count that as a success contributing to the overall AUC score.</p>
</li>
</ul>
</li>
</ul>
<p>However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:</p>
<ul>
<li>Scale invariance is not always desirable. </li>
<li><p>For example, sometimes we really do need <strong>well calibrated probability outputs</strong>, and AUC won’t tell us about that.</p>
</li>
<li><p><strong>Classification-threshold invariance</strong> is not always desirable. In cases where there are <strong>wide disparities in the cost of false negatives vs. false positives</strong>, it may be critical to minimize one type of classification error.</p>
</li>
</ul>
<h3 id="When-AUC-is-not--suitable?">When AUC is not  suitable?<a class="anchor-link" href="#When-AUC-is-not--suitable?">&#182;</a></h3><p>For example, when doing email spam detection,</p>
<ul>
<li>you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). </li>
<li><p>AUC isn't a useful metric for this type of optimization.</p>
</li>
<li><p>In practice, if you have a "perfect" classifier with an AUC of 1.0, you should be suspicious, as it likely indicates a bug in your model.</p>
</li>
<li><p>For example, you may have overfit to your training data, or the label data may be replicated in one of your features</p>
</li>
</ul>
<ul>
<li><p>Without an explicit cost of error model (cost of false positives and separate cost of false negatives) you should always be suspicious of a single number summary of a classifier performance (be it accuracy, AUC, F1 or so on).</p>
</li>
<li><p>We in fact prefer using both precision and recall. If you insist on a single number: the F1 is a good heuristic measure of classifier quality, as it at least incorporates our operational choice of score threshold into the quality assessment.</p>
</li>
<li><p>The ROC curve is useful tool designing a classifier from a scoring function (though I prefer the “double hump graph”), but once you have chosen a threshold the performance of the other classifiers (induced by choosing different thresholds) are irrelevant to assessing the performance of the classifier you have settled on.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Sources: <br>
<a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc">https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc</a> <br></p>
<p><a href="https://stackoverflow.com/questions/19984957/scikit-predict-default-threshold">https://stackoverflow.com/questions/19984957/scikit-predict-default-threshold</a> <br></p>
<p><a href="https://www.kaggle.com/kevinarvai/fine-tuning-a-classifier-in-scikit-learn">https://www.kaggle.com/kevinarvai/fine-tuning-a-classifier-in-scikit-learn</a> <br>
<a href="https://stackoverflow.com/questions/31417487/sklearn-logisticregression-and-changing-the-default-threshold-for-classification?rq=1">https://stackoverflow.com/questions/31417487/sklearn-logisticregression-and-changing-the-default-threshold-for-classification?rq=1</a> <br></p>
<p><a href="https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65">https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65</a> <br></p>
<p><a href="https://people.inf.elte.hu/kiss/11dwhdm/roc.pdf">https://people.inf.elte.hu/kiss/11dwhdm/roc.pdf</a> <br></p>

</div>
</div>
</div>
 

