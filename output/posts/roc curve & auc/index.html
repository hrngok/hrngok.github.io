<!DOCTYPE html>
<html prefix="        og: http://ogp.me/ns# article: http://ogp.me/ns/article#     " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>ROC Curve &amp; AUC | Data to Wisdom</title>
<link href="https://fonts.googleapis.com/css?family=Bitter:400,400i,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="https://example.com/posts/roc%20curve%20%26%20auc/">
<!--[if lt IE 9]><script src="../../assets/js/html5shiv-printshiv.min.js"></script><![endif]--><meta name="author" content="Harun">
<link rel="prev" href="../metrics3/" title="Metrics - Precision, Recall, F1 Score" type="text/html">
<link rel="next" href="../bikeshare/" title="Feature Selection by Visualization" type="text/html">
<meta property="og:site_name" content="Data to Wisdom">
<meta property="og:title" content="ROC Curve &amp; AUC">
<meta property="og:url" content="https://example.com/posts/roc%20curve%20%26%20auc/">
<meta property="og:description" content="We will talk about the classification models performance evaluation methods: 
Receiver Operating Characteristic(ROC) Curve and
Area Under the ROC Curve (AUC)











Classifiers generally don't ret">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2018-10-16T22:08:04+02:00">
<meta property="article:tag" content="area under the roc curve">
<meta property="article:tag" content="auc">
<meta property="article:tag" content="roc curve">
</head>
<body>
    <a href="#page-content" class="sr-only sr-only-focusable">Skip to main content</a>
    <section class="social"><ul>
<li><a href="../../index.html" title="Home"><i class="fa fa-home"></i></a></li>
            <li><a href="../../pages/about-me/index.html" title="About me"><i class="fa fa-user"></i></a></li>
            <li><a href="../../archive.html" title="Archives"><i class="fa fa-folder-open"></i></a></li>
            <li><a href="../../categories/index.html" title="Tags"><i class="fa fa-tags"></i></a></li>
    
    

        </ul></section><section class="page-content"><div class="content" rel="main">
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="../roc%20curve%20%26%20auc/" class="u-url">ROC Curve &amp; AUC</a></h1>

        <div class="metadata">
            <p class="dateline"><a href="../roc%20curve%20%26%20auc/" rel="bookmark"><i class="fa fa-clock"></i> <time class="published dt-published" datetime="2018-10-16T22:08:04+02:00" itemprop="datePublished" title="2018-10-16 22:08">2018-10-16 22:08</time></a></p>
            <p class="byline author vcard"> <i class="fa fa-user"></i> <span class="byline-name fn" itemprop="author">
                    Harun
            </span></p>
                <p class="commentline"><i class="far fa-comment"></i>
        


            
        </p>
<p class="sourceline"><a href="../roc%20curve%20%26%20auc/index.ipynb" class="sourcelink"><i class="fa fa-file-code"></i> Source</a></p>

            

            
    <div class="tags">
<h3 class="metadata-title">
<i class="fa fa-tags"></i> Tags:</h3>
        <ul itemprop="keywords" class="tags-ul">
<li><a class="tag p-category" href="../../categories/area-under-the-roc-curve/" rel="tag">area under the roc curve</a></li>
            <li><a class="tag p-category" href="../../categories/auc/" rel="tag">auc</a></li>
            <li><a class="tag p-category" href="../../categories/roc-curve/" rel="tag">roc curve</a></li>
        </ul>
</div>

        </div>
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>We will talk about the classification models performance evaluation methods: <ul>
<li>Receiver Operating Characteristic(ROC) Curve and</li>
<li>Area Under the ROC Curve (AUC)</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Classifiers generally don't return a simple “Yes-or-No” answer. </li>
<li>Mostly, a classification procedure will return a score along a range</li>
<li>Ideally, the positive instances score towards one end of the scale, and the negative examples towards the other end. </li>
<li>
<p>It is up to the analyst to set a <strong>threshold</strong> on that score that separates what is considered a POSITIVE class or a NEGATIVE class.</p>
</li>
<li>
<p>The ROC Curve is a tool that helps to set the <strong>best threshold</strong>.</p>
</li>
</ul>
<p><img src="../../images/roc1.jpg" alt="roc1"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<!-- TEASER_END -->
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>To understand the concept, here is a simple example of Logistic Regression for classification of mice as <strong>obese</strong> or <strong>not obese</strong> reagarding their weights. </li>
<li>X-axis shows the <strong>weights</strong>
</li>
<li>Y-axis shows the <strong>probabilities</strong> of the target variable (being obese)</li>
</ul>
<p><img src="../../images/roc2.jpg" alt="roc2"></p>
<p>Example is taken from <a href="https://www.youtube.com/watch?v=xugjARegisk">this youtube channel</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>We need to turn these probabilities into classification as <strong>obese</strong> or <strong>not obese</strong>
</li>
<li>One way to classify the mice is to set a threshold at $0.5$</li>
<li>The samples with the probability over $0.5$ will be classified as <strong>obese</strong> and the ones below will be classified as <strong>non-obese</strong>
</li>
</ul>
<p><img src="../../images/roc3.jpg" alt="roc3"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
<p>To evaluate the efectiveness of the Logistic Regression with the classification <strong>threshold set to 0.5</strong> we can test it with new samples (mice) that we know obese or not obese</p>
</li>
<li>
<p>The blue ones are <strong>actual obese</strong> and the red ones are <strong>actual not obese</strong></p>
</li>
</ul>
<p><img src="../../images/roc4.png" alt="roc4"></p>
<ul>
<li>When we look at the plot we see that <ul>
<li>1 <strong>actual obese</strong>(blue) classified as <strong>not obese</strong> (under the threshold)</li>
<li>1 <strong>actual not obese</strong>(red) classified as <strong>obese</strong> (over the threshold)</li>
<li>3 actual obese and 3 actual not obese classified correctly</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let's create a confusion matrix from these results</li>
</ul>
<p><img src="../../images/roc5.png" alt="roc5"></p>
<ul>
<li>With these results we can evaluate the performance of this Logistic Regression with the threshold set to $0.5$<ul>
<li>
<code>Sensitivity (Recall)</code>= $0.75$</li>
<li>
<code>Specifity</code> = $0.75$</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>What if we set the threshold to $0.1$ in order to be able classify all the <strong>actual obese</strong> samples as <strong>obese</strong>
<img src="../../images/roc6.png" alt="roc6">
</li>
</ul>
<ul>
<li>When we move the threshold from $0.5$ to $0.1$, or visually thinking when we move the threshold bar towards the bottom<ul>
<li>we classify the samples which stay above the bar are as POSITIVE(1) and the ones below the bar as NEGATIVE(0)</li>
<li>we give more chance to capture all the actual POSITIVE (actual obese) samples but </li>
<li>some actual NEGATIVEs also stayed over the bar in the positive zone as a result</li>
<li>we increased the number of <strong>TRUE</strong>(ly) predicted <strong>POSITIVE</strong>s and  <strong>FALSE</strong>(ly) predicted as <strong>POSITIVE</strong>s</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let's create a confusion matrix from these results</li>
</ul>
<p><img src="../../images/roc7.png" alt="roc7"></p>
<ul>
<li>With these results we can evaluate the performance of the Logistic Regression with the threshold set to 
$0.1$<ul>
<li>
<code>Sensitivity (Recall)</code>= $1.00$</li>
<li>
<code>Specifity</code>= $0.5$</li>
</ul>
</li>
</ul>
<ul>
<li>We notice that True Positive Rate (Sensitivity) increased and True Negative Rate (Specifity) decreased</li>
<li>There is a trade of between True Positive Rate and True Negative Rate </li>
<li>When we change the threshold generally one of them increases and the other decreases</li>
</ul>
<h4 id="How-can-we-find-the-optimal-threshold?">How can we find the optimal threshold?<a class="anchor-link" href="../roc%20curve%20%26%20auc/#How-can-we-find-the-optimal-threshold?">¶</a>
</h4>
<ul>
<li>We can't compute confusion matrix for all the threshoslds but libraries like Sklearn can bring us the ROC curves</li>
<li>ROC curves shows the results of all the possible thresholds</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Receiver-Operating-Characteristics-(ROC-CURVE)">Receiver Operating Characteristics (ROC CURVE)<a class="anchor-link" href="../roc%20curve%20%26%20auc/#Receiver-Operating-Characteristics-(ROC-CURVE)">¶</a>
</h3>
<ul>
<li>
<p>(Name comes from radar signals Receiver Operating Characteristics)</p>
</li>
<li>
<p>A graph</p>
<ul>
<li>
<p>summarizes the performance of a binary classifier (two classes 1 or 0, positive or negative) <strong>over all thresholds</strong></p>
</li>
<li>
<p>compares and visualizes the tradeoff between the model’s sensitivity and specificity.</p>
</li>
<li>generated by plotting <strong>True Positive Rate</strong> (y-axis) against <strong>False Positive Rate</strong> (x-axis)</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>The values (x,y) of each point on the ROC curve changes by the classification threshold used for assigning observations to a given class</p>
</li>
<li>
<p>We cannot compute the ROC curve from a confusion matrix.</p>
</li>
</ul>
<ul>
<li>ROC curves have an attractive property: they are <strong>insensitive to changes in class distribution</strong>. </li>
<li>If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. </li>
</ul>
<ul>
<li>Any performance metric that uses values from both columns (positive and negative) will be inherently sensitive to class skews. </li>
<li>
<p>Metrics such as <strong>accuracy, precision, lift</strong> and <strong>F-score</strong> use values from both columns of the confusion matrix.</p>
</li>
<li>
<p>As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not.</p>
</li>
</ul>
<ul>
<li>ROC graphs are based upon True Positive Rate and False Positive Rate, in which each dimension is a strict columnar ratio, so do not depend on class distributions.</li>
</ul>
<ul>
<li>ROC curves can only be used to assess classiﬁers that return some conﬁdence score (or a probability) of prediction. - For example, logistic regression, neural networks, and decision trees (and ensemble models based on decision trees) can be assessed using ROC curves. </li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><code>True Positive Rate= True Positive / All Positives</code></li>
<li><code>False Positive Rate= False Positive/ All Negatives</code></li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Let's try to answer the question: <strong>"How can we find the optimal threshold?"</strong> by checking 3 thresholds (points) on the ROC curve below.</li>
</ul>
<p><img src="../../images/roc8.png" alt="roc8"></p>
<p>(By the way, any ROC curve generated from a finite set of samples is actually a step function, like above, which approaches a true curve as the number of instances approaches infinity)</p>
<ul>
<li>Threshold1: At the bottom-left, point $(0,0)$ (The threshold bar is at the top)<ul>
<li>False Positive Rate (FPR):$0$ This is great. The classifier did not make identified any actual Negative sample as Positive </li>
<li>True Positive Rate (TPR): $0$ This is awful. The classifier could not catch any of the True Positive samples</li>
</ul>
</li>
</ul>
<ul>
<li>Threshold2: At the top-left corner, point $(0.4, 0.8)$   <ul>
<li>FPR:$0.4$ Classifier identified some negative samples as positive</li>
<li>TPR:$0.8$ Classifier showed a good performance on capturing the actual positives</li>
</ul>
</li>
</ul>
<ul>
<li>Threshold3: At the top-right, point $(1.0, 1.0)$ (The thereshold bar is at the bottom)<ul>
<li>FPR:$1.0$ This is awful. Classifier identified all the actual negative samples as positive</li>
<li>TPR:$1.0$ Classifier showed a good performance on capturing all the actual positives</li>
</ul>
</li>
</ul>
<ul>
<li>
<p>So, the ideal point is therefore the top-left corner of the plot: false positives are close to $0$ and true positives are close $1$.</p>
</li>
<li>
<p>Now, we know how to choose the optimal threshold for a classifier by ROC curve but <strong>how can we choose the best performing model with ROC curves?</strong>:</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>After plotting the ROC curves of some models we inspect<ul>
<li>the top-left corner values on the curves and</li>
<li>the “steepness” of the curve, as this describes the <strong>maximization of the true positive rate</strong> while <strong>minimizing the false positive rate</strong>.</li>
</ul>
</li>
</ul>
<p><img src="../../images/roc9.png" alt="roc9"></p>
<ul>
<li>This leads to another metric, <strong>Area Under the Curve (AUC)</strong>, which is a computation of the relationship between false positives and true positives</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Area-Under-the-ROC-Curve-(AUC)">Area Under the ROC Curve (AUC)<a class="anchor-link" href="../roc%20curve%20%26%20auc/#Area-Under-the-ROC-Curve-(AUC)">¶</a>
</h3>
<ul>
<li>
<p>To compare the optimal performances of different classifiers we can use the size of the area of each classifiers ROC curves</p>
</li>
<li>
<p>As the picture displays the bigger area under the curve means the model of that curve can classify the actual positive and actual negatives better.</p>
</li>
<li>
<p>Think of the red and green curves(distributions) like this:</p>
<ul>
<li>Firstly, they are not ROC curves, ROC curves are on the right side</li>
<li>We gave a validation dataset to some models and</li>
<li>models gave us the predict probability of each sample (e.g Sklearn with <code>predict_proba</code>)</li>
<li>Since we know the labels of them. We assosiated <strong>green for the positives</strong> and <strong>red for the negatives</strong>
</li>
<li>We noticed that some models could separate two classes better than other. </li>
<li>The more overlapping the worse classification performance</li>
</ul>
</li>
</ul>
<p><img src="../../images/roc10.png" alt="roc10"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="How-to-Interpret-AUC?">How to Interpret AUC?<a class="anchor-link" href="../roc%20curve%20%26%20auc/#How-to-Interpret-AUC?">¶</a>
</h4>
<p><img src="../../images/roc11.png" alt="roc11"></p>
<ul>
<li>AUC provides an <strong>aggregate measure</strong> of performance <strong>across all possible classification thresholds</strong>. </li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's try to interpret a model's AUC score which has a ROC curve like below:</p>
<p><img src="../../images/roc12.png" alt="roc12"></p>
<ul>
<li>
<p>For all the thresholds the values of <strong>True Positive Rate (TPR)</strong> and the <strong>False Positive Rate(FPR)</strong> (the proportion of negative examples predicted incorrectly) will be equal on the curve (The diagonal line <strong>y=x</strong>)</p>
</li>
<li>
<p>In order to get away from this diagonal into the upper triangular region, the classifier must exploit some information in the data.</p>
</li>
</ul>
<ul>
<li>
<p>Any classifier that appears in the lower right triangle performs worse than random guessing.</p>
</li>
<li>
<p>This triangle is therefore usually empty in ROC graphs.</p>
</li>
</ul>
<ul>
<li>
<p>If we negate a classifier that is, reverse its classification decisions on every instance its true positive classifications become false negative mistakes, and its false positives become true negatives.</p>
</li>
<li>
<p>Therefore, any classifier that produces a point in the lower right triangle can be negated to produce a point in the upper left triangle.</p>
</li>
</ul>
<ul>
<li>
<p>Any classifier on the diagonal may be said to have no information about the class.</p>
</li>
<li>
<p>A classifier below the diagonal may be said to have useful information, but it is applying the information incorrectly</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>We look for the model performance where True Positive Rate is significantly higher than False Positive Rate.</li>
<li>This means that our ROC curve should satisfy the condition <strong>y &gt; x</strong>
</li>
<li>This is the region of upper triangle</li>
</ul>
<p><img src="../../images/roc13.png" alt="roc13"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Why-is-AUC-a-desirable-metric?">Why is AUC a desirable metric?<a class="anchor-link" href="../roc%20curve%20%26%20auc/#Why-is-AUC-a-desirable-metric?">¶</a>
</h4>
<ul>
<li>AUC is <strong>scale-invariant</strong>. <ul>
<li>It measures how well predictions are ranked, rather than their absolute values.</li>
<li>AUC is based on the relative predictions, so any transformation of the predictions that preserves the relative ranking has no effect on AUC. </li>
</ul>
</li>
</ul>
<ul>
<li>
<p>AUC is <strong>classification-threshold-invariant</strong>.</p>
<ul>
<li>
<p>It measures the quality of the model's predictions irrespective of what classification threshold is chosen.</p>
</li>
<li>
<p>The amount of spread between predictions does not actually impact AUC. Even a prediction score for a randomly drawn true positive is only a tiny epsilon greater than a randomly drawn negative, that will count that as a success contributing to the overall AUC score.</p>
</li>
</ul>
</li>
</ul>
<p>However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:</p>
<ul>
<li>Scale invariance is not always desirable. </li>
<li>
<p>For example, sometimes we really do need <strong>well calibrated probability outputs</strong>, and AUC won’t tell us about that.</p>
</li>
<li>
<p><strong>Classification-threshold invariance</strong> is not always desirable. In cases where there are <strong>wide disparities in the cost of false negatives vs. false positives</strong>, it may be critical to minimize one type of classification error.</p>
</li>
</ul>
<h4 id="When-AUC-is-not--suitable?">When AUC is not  suitable?<a class="anchor-link" href="../roc%20curve%20%26%20auc/#When-AUC-is-not--suitable?">¶</a>
</h4>
<p>For example, when doing email spam detection,</p>
<ul>
<li>you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). </li>
<li>
<p>AUC isn't a useful metric for this type of optimization.</p>
</li>
<li>
<p>In practice, if you have a "perfect" classifier with an AUC of 1.0, you should be suspicious, as it likely indicates a bug in your model.</p>
</li>
<li>
<p>For example, you may have overfit to your training data, or the label data may be replicated in one of your features</p>
</li>
</ul>
<ul>
<li>
<p>Without an explicit cost of error model (cost of false positives and separate cost of false negatives) you should always be suspicious of a single number summary of a classifier performance (be it accuracy, AUC, F1 or so on).</p>
</li>
<li>
<p>We in fact prefer using both precision and recall. If you insist on a single number: the F1 is a good heuristic measure of classifier quality, as it at least incorporates our operational choice of score threshold into the quality assessment.</p>
</li>
<li>
<p>The ROC curve is useful tool designing a classifier from a scoring function (though I prefer the “double hump graph”), but once you have chosen a threshold the performance of the other classifiers (induced by choosing different thresholds) are irrelevant to assessing the performance of the classifier you have settled on.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Sources: <br><a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc">https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc</a> <br></p>
<p><a href="https://stackoverflow.com/questions/19984957/scikit-predict-default-threshold">https://stackoverflow.com/questions/19984957/scikit-predict-default-threshold</a> <br></p>
<p><a href="https://www.kaggle.com/kevinarvai/fine-tuning-a-classifier-in-scikit-learn">https://www.kaggle.com/kevinarvai/fine-tuning-a-classifier-in-scikit-learn</a> <br><a href="https://stackoverflow.com/questions/31417487/sklearn-logisticregression-and-changing-the-default-threshold-for-classification?rq=1">https://stackoverflow.com/questions/31417487/sklearn-logisticregression-and-changing-the-default-threshold-for-classification?rq=1</a> <br></p>
<p><a href="https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65">https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65</a> <br></p>
<p><a href="https://people.inf.elte.hu/kiss/11dwhdm/roc.pdf">https://people.inf.elte.hu/kiss/11dwhdm/roc.pdf</a> <br></p>

</div>
</div>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul class="pager hidden-print">
<li class="previous">
                <a href="../metrics3/" rel="prev" title="Metrics - Precision, Recall, F1 Score">Previous post</a>
            </li>
            <li class="next">
                <a href="../bikeshare/" rel="next" title="Feature Selection by Visualization">Next post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
        
        


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article><footer id="footer"><p>Contents © 2019         <a href="mailto:n.tesla@example.com">Harun</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer>
</div>
    </section><script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(2, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
